package nju.lamda.hadoop.liblinear;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.Scanner;

import nju.lamda.hadoop.CopyFileSplitEx;
import nju.lamda.svm.SvmIo;
import nju.lamda.svm.SvmOp;
import nju.lamda.svm.SvmRecord;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.util.Tool;

import de.bwaldvogel.liblinear.FeatureNode;

public class PredictProcEx extends Configured implements Tool {
	public String filenameData;
	public String filenameModel;
	public String filenameResult;
	public String workingDir;
	public int bufferSize;
	
	public static class MyMapper extends Mapper<LongWritable, Text, Text, Text> {
		private Text k = new Text();
		private Text v = new Text();

		private int numDim;
		private double bias;

		private Path pathModelFile;

		public void setup(Context context) {
			Configuration conf = context.getConfiguration();
			CopyFileSplitEx split = (CopyFileSplitEx) context.getInputSplit();
			int index = split.getIndex();
			double w[];

			try {
				pathModelFile = DistributedCache.getLocalCacheFiles(conf)[0];
				BufferedReader reader = new BufferedReader(new FileReader(
						pathModelFile.toString()));

				Scanner scanner = new Scanner(reader);
				numDim = scanner.nextInt();
				scanner.nextInt();
				bias = scanner.nextDouble();

				wDim = numDim;
				if (bias > 0) {
					wDim++;
				}
				
				scanner.close();
				reader.close();

			} catch (IOException e) {

				e.printStackTrace();
			}
		}

		public void map(LongWritable key, Text value, Context context)
				throws IOException, InterruptedException {
			String line = value.toString();
			int bpos = line.indexOf('\t');
			String id = line.substring(0, bpos);
			SvmRecord rec = SvmIo.Parse(line.substring(bpos + 1), this.numDim,
					this.bias);
			this.bufferKey[bufferIndex] = id;
			this.buffer[bufferIndex++] = rec.feature;

			if (bufferIndex == bufferSize) {
				flushBuffer(context);
			}

		}

		@Override
		public void cleanup(Context context) {
			if (bufferIndex > 0) {
				try {
					flushBuffer(context);
				} catch (IOException e) {
					e.printStackTrace();
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
			}
		}

		public void flushBuffer(Context context) throws IOException,
				InterruptedException {
			BufferedReader r = new BufferedReader(new FileReader(
					this.pathModelFile.toString()));
			r.readLine();// skip first line
			double w[] = new double[wDim];
			
			Scanner scanner = new Scanner(r);

			do {
				int label = scanner.nextInt();
				
				for (int i = 0; i < wDim; i++) {
					w[i] = scanner.nextDouble();
				}

				for (int i = 0; i < this.bufferIndex; i++) {
					FeatureNode[] fea = this.buffer[i];
					double score = SvmOp.dot(w, fea);

					this.k.set(this.bufferKey[i]);
					this.v.set(label + " " + score);

					context.write(k, v);
				}
			} while (scanner.hasNext());

			scanner.close();
			r.close();
			this.bufferIndex = 0;
		}
	}

	@Override
	public int run(String[] as) throws Exception {
		// TODO Auto-generated method stub
		return 0;
	}

}
