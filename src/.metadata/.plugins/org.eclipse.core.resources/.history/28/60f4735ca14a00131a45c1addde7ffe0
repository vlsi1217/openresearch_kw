package nju.lamda.hadoop.liblinear;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.Scanner;

import nju.lamda.hadoop.UtilsMapReduce;
import nju.lamda.svm.SvmIo;
import nju.lamda.svm.SvmOp;
import nju.lamda.svm.SvmRecord;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import de.bwaldvogel.liblinear.FeatureNode;

public class PredictProc extends Configured implements Tool {
	
	public String filenameData;
	public String filenameModel;
	public String filenameResult;
	public String workingDir;
	public int bufferSize;
	
	public static class MyMapper extends Mapper<LongWritable, Text, Text, Text> {
		private Text k = new Text();
		private Text v = new Text();

		private int numDim;
		private double bias;
		private int bufferSize;
		private FeatureNode buffer[][];
		private String bufferKey[];
		private int wDim;

		private int bufferIndex;

		private Path pathModelFile;

		public void setup(Context context) {
			Configuration conf = context.getConfiguration();
			bufferSize = conf.getInt("svm.predict.buffersize", 100000);
			
			buffer = new FeatureNode[bufferSize][];
			bufferKey = new String[bufferSize];
			bufferIndex = 0;

			try {
				pathModelFile = DistributedCache.getLocalCacheFiles(conf)[0];
				BufferedReader reader = new BufferedReader(new FileReader(
						pathModelFile.toString()));

				Scanner scanner = new Scanner(reader);
				numDim = scanner.nextInt();
				scanner.nextInt();
				bias = scanner.nextDouble();

				scanner.close();
				reader.close();

			} catch (IOException e) {

				e.printStackTrace();
			}

			wDim = numDim;
			if (bias > 0) {
				wDim++;
			}
		}

		public void map(LongWritable key, Text value, Context context)
				throws IOException, InterruptedException {
			String line = value.toString();
			int bpos = line.indexOf('\t');
			String id = line.substring(0, bpos);
			SvmRecord rec = SvmIo.Parse(line.substring(bpos + 1), this.numDim,
					this.bias);
			this.bufferKey[bufferIndex] = id;
			this.buffer[bufferIndex++] = rec.feature;

			if (bufferIndex == bufferSize) {
				flushBuffer(context);
			}

		}

		@Override
		public void cleanup(Context context) {
			if (bufferIndex > 0) {
				try {
					flushBuffer(context);
				} catch (IOException e) {
					e.printStackTrace();
				} catch (InterruptedException e) {
					e.printStackTrace();
				}
			}
		}

		public void flushBuffer(Context context) throws IOException,
				InterruptedException {
			BufferedReader r = new BufferedReader(new FileReader(
					this.pathModelFile.toString()));
			r.readLine();// skip first line
			String line = "";
			double w[] = new double[wDim];

			do {
				line = r.readLine();

				if (line == null) {
					break;
				}

				String parts[] = line.split("[\\t ]");
				int label = Integer.parseInt(parts[0]);
				for (int i = 0; i < wDim; i++) {
					w[i] = Double.parseDouble(parts[i+1]);
				}

				for (int i = 0; i < this.bufferIndex; i++) {
					FeatureNode[] fea = this.buffer[i];
					double score = SvmOp.dot(w, fea);

					this.k.set(this.bufferKey[i]);
					this.v.set(label + " " + score);

					context.write(k, v);
				}
				System.gc();
			} while (line != null);

			r.close();
			this.bufferIndex = 0;
		}
	}

	public static class MyCombiner extends Reducer<Text, Text, Text, Text> {

		private Text outputKey = new Text();
		private Text outputValue = new Text();

		@Override
		public void setup(Context context) {

		}

		public void reduce(Text key, Iterable<Text> values, Context context)
				throws IOException, InterruptedException {

			double maxscore = -99999;
			String predictLabel = "";

			for(Text val : values) {
				String l = val.toString();

				int i = l.indexOf(' ');

				String label = l.substring(0, i);
				double score = Double.parseDouble(l.substring(i + 1));

				if (score > maxscore) {
					maxscore = score;
					predictLabel = label;
				}
			}

			outputKey = key;
			outputValue.set(predictLabel + " " + maxscore);

			context.write(outputKey, outputValue);
		}
	}

	public static class MyReducer extends Reducer<Text, Text, LongWritable, Text> {

		private LongWritable outputKey = new LongWritable();
		private Text outputValue = new Text();

		@Override
		public void setup(Context context) {

		}

		public void reduce(Text key, Iterable<Text> values, Context context)
				throws IOException, InterruptedException {

			double maxscore = -99999;
			String predictLabel = "";

			for(Text val : values) {
				String l = val.toString();

				int i = l.indexOf(' ');

				String label = l.substring(0, i);
				double score = Double.parseDouble(l.substring(i + 1));

				if (score > maxscore) {
					maxscore = score;
					predictLabel = label;
				}
			}

			outputKey.set(Long.parseLong(key.toString()));
			outputValue.set(predictLabel + " " + maxscore);

			context.write(outputKey, outputValue);
		}
	}

	@Override
	public int run(String[] arg0) throws Exception {
		
		Configuration conf = getConf();
		Job job = new Job(conf,"SVM Predict Process");
		job.setJarByClass(PredictProc.class);
        job.setMapperClass(MyMapper.class);
        job.setCombinerClass(MyCombiner.class);
        job.setReducerClass(MyReducer.class);
        job.setOutputKeyClass(LongWritable.class);
        job.setMapOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);
        job.setNumReduceTasks(50);
        
        Path output = new Path(workingDir + "/" + Constants.OUTDIR_SVM_PREDICT);
        FileSystem fs = FileSystem.get(conf);
        fs.deleteOnExit(output);
        
        
        FileInputFormat.addInputPath(job, new Path(filenameData));
        FileOutputFormat.setOutputPath(job, output);
        
        FileStatus fsta = fs.getFileStatus(new Path(filenameData));
        
        FileInputFormat.setMaxInputSplitSize(job, fsta.getLen()/32);
        fs.close();
        conf = job.getConfiguration();
        
        conf.setInt("svm.predict.buffersize", this.bufferSize);
        
		DistributedCache.addCacheFile(
				(new Path(this.filenameModel)).toUri(), job.getConfiguration());
		UtilsMapReduce.AddLibraryPath(conf, new Path("lib"));
        if (!job.waitForCompletion(true))
        {
        	return 1;
        }
        
        Path pathResult = new Path(filenameResult);
        
        UtilsMapReduce.CopyResult(conf, output, pathResult, false);
        
        return 0;
	}

	public int run(String datafile, String modelfile, String resultfile,
			String workingDir, int buffersize) throws Exception
	{
		this.bufferSize = buffersize;
		this.workingDir = workingDir;
		this.filenameData = datafile;
		this.filenameModel = modelfile;
		this.filenameResult = resultfile;
		
		return ToolRunner.run(this, null);
	}
}
